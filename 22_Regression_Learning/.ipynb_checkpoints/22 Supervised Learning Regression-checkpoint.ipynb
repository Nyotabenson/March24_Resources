{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244005c2",
   "metadata": {},
   "source": [
    "# Regression Learning\n",
    "\n",
    "\n",
    "## - Regression techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8e535",
   "metadata": {},
   "source": [
    "## Table of Content \n",
    "\n",
    "### 1 Introduction to Regression:\n",
    "\n",
    "* Understand the concept of regression in machine learning\n",
    "* Differentiate between classification and regression tasks\n",
    "\n",
    "### 2 Simple Linear Regression:\n",
    "\n",
    "* Understand the concept of simple linear regression\n",
    "* Implement simple linear regression using scikit-learn\n",
    "* Evaluate the performance of a simple linear regression model\n",
    "\n",
    "### 3 Multiple Linear Regression:\n",
    "\n",
    "* Understand the concept of multiple linear regression\n",
    "* Implement multiple linear regression using scikit-learn\n",
    "* Evaluate the performance of a multiple linear regression model\n",
    "\n",
    "### 4 Polynomial Regression:\n",
    "\n",
    "* Understand the concept of polynomial regression\n",
    "* Implement polynomial regression using scikit-learn\n",
    "* Evaluate the performance of a polynomial regression model\n",
    "\n",
    "### 5 Regularization Techniques:\n",
    "\n",
    "* Understand the concept of overfitting and underfitting\n",
    "* Learn about Ridge regression (L2 regularization)\n",
    "* Learn about Lasso regression (L1 regularization)\n",
    "* Implement Ridge and Lasso regression using scikit-learn\n",
    "* Evaluate the performance of Ridge and Lasso regression models\n",
    "\n",
    "### 6 Support Vector Regression (SVR):\n",
    "\n",
    "* Understand the concept of support vector machines for regression\n",
    "* Implement support vector regression using scikit-learn\n",
    "* Evaluate the performance of a support vector regression model\n",
    "\n",
    "### 7 Decision Tree Regression:\n",
    "\n",
    "* Understand the concept of decision trees for regression\n",
    "* Implement decision tree regression using scikit-learn\n",
    "* Evaluate the performance of a decision tree regression model\n",
    "\n",
    "### 8 Random Forest Regression:\n",
    "\n",
    "* Understand the concept of ensemble learning and random forests for regression\n",
    "* Implement random forest regression using scikit-learn\n",
    "* Evaluate the performance of a random forest regression model\n",
    "\n",
    "### 9  Model Evaluation Metrics for Regression:\n",
    "\n",
    "* Learn about various evaluation metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE)\n",
    "* Understand the concept of R-squared and Adjusted R-squared\n",
    "\n",
    "### 10  Model Selection and Hyperparameter Tuning:\n",
    "\n",
    "* Learn how to split datasets into training, validation, and testing sets\n",
    "* Understand the concept of cross-validation\n",
    "* Learn about grid search and randomized search for hyperparameter tuning\n",
    "* Implement model selection and hyperparameter tuning using scikit-learn \n",
    "\n",
    "### 11 Assignment: Predicting House Prices using Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0ae9f",
   "metadata": {},
   "source": [
    "## Getting Started with scikit-learn\n",
    "\n",
    "### Installing scikit-learn:\n",
    "\n",
    "Scikit-learn is a popular Python library for machine learning that provides simple and efficient tools for data mining and data analysis. To install scikit-learn, you can use the following command with pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c54c153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\bency\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\bency\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\bency\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\bency\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bency\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda3cc4",
   "metadata": {},
   "source": [
    "## 1 Introduction to Regression:\n",
    "\n",
    "Regression is a supervised learning technique in machine learning that focuses on predicting continuous numerical outcomes based on input features. The main objective of regression is to establish a relationship between the independent variables (input features) and the dependent variable (output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b4782",
   "metadata": {},
   "source": [
    "### Understand the concept of regression in machine learning:\n",
    "\n",
    "In regression, we attempt to fit a model that best represents the relationship between the dependent and independent variables. This model is then used to make predictions on new, unseen data. Linear regression is a common type of regression where the relationship between the variables is modeled as a straight line.\n",
    "\n",
    "\n",
    "Examples of regression problems include predicting housing prices, estimating stock prices, or forecasting product sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab630d",
   "metadata": {},
   "source": [
    "### Differentiate between classification and regression tasks:\n",
    "\n",
    "While both classification and regression are types of supervised learning, they have different goals:\n",
    "\n",
    "* **Classification**: \n",
    "    The objective is to predict a category or class label for a given input. Examples include spam detection, image recognition, and medical diagnosis. Classification models predict discrete outcomes.\n",
    "\n",
    "    \n",
    "* **Regression**: \n",
    "    The objective is to predict a continuous numerical value for a given input. Examples include predicting housing prices, estimating stock prices, and forecasting product sales. Regression models predict continuous outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1084a2",
   "metadata": {},
   "source": [
    "In summary, regression is a supervised learning technique used to predict continuous numerical outcomes, while classification focuses on predicting discrete categorical outcomes. Both techniques play a crucial role in machine learning and can be applied to various real-world problems depending on the nature of the data and the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd01975",
   "metadata": {},
   "source": [
    "## 2 Simple Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e86de",
   "metadata": {},
   "source": [
    "Simple linear regression is a basic regression technique that models the relationship between a single independent variable (input feature) and a dependent variable (output) as a straight line. The goal is to find the best-fitting line that minimizes the error between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508ea344",
   "metadata": {},
   "source": [
    "### Understand the concept of simple linear regression:\n",
    "    \n",
    "    \n",
    "The equation for simple linear regression is:\n",
    "    \n",
    "    \n",
    "\n",
    "y = b0 + b1 * x\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* y is the dependent variable (output)\n",
    "* x is the independent variable (input feature)\n",
    "* b0 is the y-intercept\n",
    "* b1 is the slope of the line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57e848",
   "metadata": {},
   "source": [
    "The objective is to find the optimal values for b0 and b1 that minimize the error between the predicted values and the actual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70280de",
   "metadata": {},
   "source": [
    "### Implement simple linear regression using scikit-learn:\n",
    "\n",
    "Here's a simple example of implementing simple linear regression using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b823c4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create sample data\n",
    "X = np.array([[1], [2], [3], [4], [5]])#input variable\n",
    "y = np.array([2, 4, 6, 8, 10])#output var\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b100e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ab851d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe6e3539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fe6ed4b5d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634261b",
   "metadata": {},
   "source": [
    "### Evaluate the performance of a simple linear regression model:\n",
    "\n",
    "To evaluate the performance of a simple linear regression model, you can use various metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or R-squared. In the example above, we used MSE as the evaluation metric. A lower MSE indicates a better fit of the model to the data. Other metrics may be more appropriate depending on the specific problem and data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44422867",
   "metadata": {},
   "source": [
    "## 3 Multiple Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78589b77",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that models the relationship between multiple independent variables (input features) and a dependent variable (output). The goal is to find the best-fitting hyperplane that minimizes the error between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99018e",
   "metadata": {},
   "source": [
    "### Understand the concept of multiple linear regression:\n",
    "\n",
    "\n",
    "The equation for multiple linear regression is:\n",
    "\n",
    "    \n",
    "y = b0 + b1 * x1 + b2 * x2 + ... + bn * xn\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "    \n",
    "* y is the dependent variable (output)\n",
    "* x1, x2, ..., xn are the independent variables (input features)\n",
    "* b0 is the y-intercept\n",
    "* b1, b2, ..., bn are the coefficients of the independent variables\n",
    "\n",
    "The objective is to find the optimal values for b0, b1, b2, ..., bn that minimize the error between the predicted values and the actual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836324f",
   "metadata": {},
   "source": [
    "### Implement multiple linear regression using scikit-learn:\n",
    "\n",
    "Here's a simple example of implementing multiple linear regression using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61f0e17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'x1': [1, 2, 3, 4, 5],\n",
    "    'x2': [2, 3, 4, 5, 6],\n",
    "    'y': [3, 5, 7, 9, 11]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['x1', 'x2']]\n",
    "y = df['y']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2037f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.]\n",
      "1    5\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cf238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41642aac",
   "metadata": {},
   "source": [
    "### Evaluate the performance of a multiple linear regression model:\n",
    "\n",
    "\n",
    "Evaluating the performance of a multiple linear regression model is similar to simple linear regression. You can use metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, or Adjusted R-squared to measure the performance. The choice of evaluation metric(s) depends on the specific problem and data distribution.\n",
    "\n",
    "\n",
    "\n",
    "In the example above, we used MSE as the evaluation metric. However, you can also calculate other metrics using the sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f0508",
   "metadata": {},
   "source": [
    "## 4 Polynomial Regression:\n",
    "\n",
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial. Polynomial regression can model more complex relationships than simple linear regression.\n",
    "\n",
    "\n",
    "\n",
    "The degree of a polynomial is just a fancy way of saying how many times the variable (usually x) is multiplied by itself. For example, if we have a polynomial with the degree of 2, that means that x is multiplied by itself two times.\n",
    "\n",
    "\n",
    "So, when we talk about an nth-degree polynomial, we mean a polynomial that has a degree of n. It might look something like this:\n",
    "\n",
    "\n",
    "P(x) = 5x^3 + 2x^2 - 3x + 7\n",
    "\n",
    "\n",
    "This is an example of a third-degree polynomial, because x is multiplied by itself three times. The \"nth-degree\" just means that we don't know what the degree is yet, but we know it's some number n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1f75f",
   "metadata": {},
   "source": [
    "### Understand the concept of polynomial regression:\n",
    "\n",
    "    \n",
    "In polynomial regression, the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial. The equation for polynomial regression with a single independent variable is:\n",
    "\n",
    "    \n",
    "y = b0 + b1 * x + b2 * x^2 + ... + bn * x^n\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* y is the dependent variable (output)\n",
    "* x is the independent variable (input feature)\n",
    "* b0 is the constant term\n",
    "* b1, b2, ..., bn are the coefficients of the independent variable's powers\n",
    "\n",
    "\n",
    "The objective is to find the optimal values for b0, b1, b2, ..., bn that minimize the error between the predicted values and the actual data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb4216",
   "metadata": {},
   "source": [
    "### Implement polynomial regression using scikit-learn:\n",
    "\n",
    "Here's a simple example of implementing polynomial regression using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dea645f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.262177448353619e-29\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'x': [1, 2, 3, 4, 5],\n",
    "    'y': [1, 4, 9, 16, 25]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['x']]\n",
    "y = df['y']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create PolynomialFeatures object with the desired degree\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Transform the input features to polynomial features\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the transformed training data\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions on the transformed testing data\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c5044",
   "metadata": {},
   "source": [
    "### Evaluate the performance of a polynomial regression model:\n",
    "\n",
    "Evaluating the performance of a polynomial regression model is similar to simple and multiple linear regression. You can use metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, or Adjusted R-squared to measure the performance. The choice of evaluation metric(s) depends on the specific problem and data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b9093",
   "metadata": {},
   "source": [
    "# 5 Regularization Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588b2b4",
   "metadata": {},
   "source": [
    "Regularization techniques are used in regression models to prevent overfitting, improve generalization, and balance model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b6902",
   "metadata": {},
   "source": [
    "### Understand the concept of overfitting and underfitting:\n",
    "    \n",
    "* **Overfitting**: When a model performs well on the training data but poorly on the testing data, it is said to be overfitting. This occurs when the model captures the noise in the data, fitting it too closely, and thus fails to generalize well to new data.\n",
    "\n",
    "    \n",
    "* **Underfitting**: When a model performs poorly on both the training and testing data, it is said to be underfitting. This occurs when the model fails to capture the underlying patterns in the data, often due to excessive simplicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d33a68",
   "metadata": {},
   "source": [
    "### Ridge regression (L2 regularization):\n",
    "\n",
    "Ridge regression is a technique that adds an L2 penalty term to the linear regression objective function. This penalty term is the sum of the squared coefficients multiplied by a regularization parameter (alpha). Ridge regression helps prevent overfitting by shrinking the coefficients, thereby reducing model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb138ee",
   "metadata": {},
   "source": [
    "### Lasso regression (L1 regularization):\n",
    "\n",
    "\n",
    "Lasso regression is another regularization technique that adds an L1 penalty term to the linear regression objective function. The L1 penalty term is the sum of the absolute values of the coefficients multiplied by a regularization parameter (alpha). Lasso regression not only helps prevent overfitting but can also induce sparsity in the model, setting some coefficients to zero and effectively performing feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd117e",
   "metadata": {},
   "source": [
    "### Implement Ridge and Lasso regression using scikit-learn:\n",
    "\n",
    "Here's a simple example of implementing Ridge and Lasso regression using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d4c7642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Ridge): 0.018261504747991222\n",
      "Mean Squared Error (Lasso): 0.32653061224489766\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'x1': [1, 2, 3, 4, 5],\n",
    "    'x2': [2, 3, 4, 5, 6],\n",
    "    'y': [3, 5, 7, 9, 11]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['x1', 'x2']]\n",
    "y = df['y']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Ridge and Lasso regression models\n",
    "ridge = Ridge(alpha=1)\n",
    "lasso = Lasso(alpha=1)\n",
    "\n",
    "# Fit the models on the training data\n",
    "ridge.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(\"Mean Squared Error (Ridge):\", mse_ridge)\n",
    "print(\"Mean Squared Error (Lasso):\", mse_lasso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25278f9e",
   "metadata": {},
   "source": [
    "### Evaluate the performance of Ridge and Lasso regression models:\n",
    "\n",
    "Evaluating the performance of Ridge and Lasso regression models is similar to other regression models. You can use metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, or Adjusted R-squared to measure the performance. The choice of evaluation metric(s) depends on the specific problem and data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4059431",
   "metadata": {},
   "source": [
    "### 6 Support Vector Regression (SVR):\n",
    "\n",
    "Support Vector Regression is an extension of Support Vector Machines (SVM) for solving regression problems. The goal of SVR is to find a function that best approximates the relationship between input features and output values while considering a margin of tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e422075",
   "metadata": {},
   "source": [
    "### Understand the concept of support vector machines for regression:\n",
    "\n",
    "\n",
    "In SVR, the objective is to fit a hyperplane that has a maximum margin while allowing some tolerance for the error in the predicted values. The margin of tolerance is determined by a parameter called epsilon (ε). SVR tries to minimize the error within this margin while also minimizing the model's complexity by penalizing large coefficients.\n",
    "\n",
    "\n",
    "There are different types of SVR, such as linear, polynomial, and radial basis function (RBF) kernel. The choice of kernel and its parameters can significantly impact the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbba165",
   "metadata": {},
   "source": [
    "### Implement support vector regression using scikit-learn:\n",
    "\n",
    "Here's a simple example of implementing support vector regression using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b991136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 50.44443474343727\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'x': [1, 2, 3, 4, 5],\n",
    "    'y': [1, 4, 9, 16, 25]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['x']]\n",
    "y = df['y']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a support vector regression model (RBF kernel)\n",
    "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5e5f9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89bc6936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.10242457])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ab2e9",
   "metadata": {},
   "source": [
    "### Evaluate the performance of a support vector regression model:\n",
    "\n",
    "Evaluating the performance of a support vector regression model is similar to other regression models. You can use metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, or Adjusted R-squared to measure the performance. The choice of evaluation metric(s) depends on the specific problem and data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797bef1",
   "metadata": {},
   "source": [
    "## 7 Decision Tree Regression:\n",
    "\n",
    "Decision Tree Regression is a type of regression model that uses a tree-like structure to represent the relationship between input features and output values. It recursively splits the data into subsets based on the input features, aiming to minimize the variance of the output values in each subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105123a0",
   "metadata": {},
   "source": [
    "### Understand the concept of decision trees for regression:\n",
    "\n",
    "A decision tree for regression works by recursively splitting the data into subsets based on the input features. The splitting criteria aim to minimize the variance of the output values in each subset. At each leaf node of the tree, the predicted value is the average of the output values in that subset. Decision trees can be prone to overfitting, especially when they grow too deep, and may require pruning or limiting the maximum depth to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46588fd2",
   "metadata": {},
   "source": [
    "### Implement decision tree regression using scikit-learn:\n",
    "\n",
    "Here's a simple example of implementing decision tree regression using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e551647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'x': [1, 2, 3, 4, 5],\n",
    "    'y': [1, 4, 9, 16, 25]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['x']]\n",
    "y = df['y']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree regression model\n",
    "dtree = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e5930",
   "metadata": {},
   "source": [
    "### Evaluate the performance of a decision tree regression model:\n",
    "\n",
    "Evaluating the performance of a decision tree regression model is similar to other regression models. You can use metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, or Adjusted R-squared to measure the performance. The choice of evaluation metric(s) depends on the specific problem and data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c63bc",
   "metadata": {},
   "source": [
    "### 8 Random Forest Regression:\n",
    "\n",
    "Random Forest Regression is an ensemble learning method that combines multiple decision trees to improve the model's performance and generalization. Each decision tree in the ensemble is built using a random subset of features and data points, which helps reduce the correlation between the trees and increase the model's robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd3988",
   "metadata": {},
   "source": [
    "### Understand the concept of ensemble learning and random forests for regression:\n",
    "\n",
    "Ensemble learning is a technique that combines multiple weak models to create a more powerful and accurate model. In the case of Random Forest Regression, multiple decision trees are combined to form an ensemble.\n",
    "\n",
    "\n",
    "\n",
    "A random forest works by constructing multiple decision trees, each trained on a random subset of the input features and data points. The random subsets are obtained using bootstrapping, which involves sampling with replacement. The final prediction of the random forest is obtained by averaging the predictions of all the individual decision trees in the ensemble.\n",
    "\n",
    "\n",
    "\n",
    "This approach reduces the correlation between the trees and helps mitigate overfitting, making random forests more robust and accurate compared to individual decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacceda",
   "metadata": {},
   "source": [
    "### Implement random forest regression using scikit-learn:\n",
    "\n",
    "Here's a simple example of implementing random forest regression using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b8fce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.008099999999999975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'x': [1, 2, 3, 4, 5],\n",
    "    'y': [1, 4, 9, 16, 25]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['x']]\n",
    "y = df['y']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a random forest regression model\n",
    "rforest = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rforest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rforest.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ef336f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.91])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7df77a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0359336a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008099999999999975"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4-3.91)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77929c",
   "metadata": {},
   "source": [
    "### Evaluate the performance of a random forest regression model:\n",
    "\n",
    "Evaluating the performance of a random forest regression model is similar to other regression models. You can use metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, or Adjusted R-squared to measure the performance. The choice of evaluation metric(s) depends on the specific problem and data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0a7ee",
   "metadata": {},
   "source": [
    "## 9 Model Evaluation Metrics for Regression:\n",
    "\n",
    "When evaluating the performance of regression models, various metrics can be used to measure how well the model fits the data and generalizes to new data points. Some common evaluation metrics for regression include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, and Adjusted R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e33dc2",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE):\n",
    "\n",
    "\n",
    "MAE measures the average absolute difference between the predicted values and the actual values. It is a simple and intuitive metric that gives an idea of the average magnitude of the errors.\n",
    "\n",
    "\n",
    "\n",
    "Formula: MAE = (1/n) * Σ|y_true - y_pred|, \n",
    "\n",
    "\n",
    "where \n",
    "* n is the number of samples, \n",
    "* y_true are the true values, and \n",
    "* y_pred are the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794e0c5",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE):\n",
    "\n",
    "\n",
    "MSE measures the average squared difference between the predicted values and the actual values. It emphasizes larger errors by squaring them, making it more sensitive to outliers than MAE.\n",
    "\n",
    "\n",
    "\n",
    "Formula: MSE = (1/n) * Σ(y_true - y_pred)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347c5f9",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE):\n",
    "\n",
    "\n",
    "RMSE is the square root of the MSE. It represents the standard deviation of the residuals, which are the differences between the predicted values and the actual values. Like MSE, it is sensitive to outliers but has the same unit as the original data, making it easier to interpret.\n",
    "\n",
    "\n",
    "Formula: RMSE = √(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9442835",
   "metadata": {},
   "source": [
    "### R-squared:\n",
    "    \n",
    "    \n",
    "R-squared, also known as the coefficient of determination, measures the proportion of variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with higher values indicating a better fit. An R-squared value of 1 indicates that the model explains all the variability in the data, while a value of 0 indicates that the model does not explain any variability.\n",
    "\n",
    "\n",
    "Formula: R^2 = 1 - (Σ(y_true - y_pred)^2) / (Σ(y_true - y_mean)^2), \n",
    "\n",
    "where \n",
    "\n",
    "* y_mean is the mean of the true values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d7e75",
   "metadata": {},
   "source": [
    "### Adjusted R-squared:\n",
    "\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables (features) in the model. It is useful when comparing models with different numbers of features, as it penalizes models that include irrelevant features. Unlike R-squared, Adjusted R-squared can decrease when adding a new feature that does not improve the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "Formula: Adjusted R^2 = 1 - [(1 - R^2) * (n - 1) / (n - k - 1)], \n",
    "    \n",
    "where \n",
    "\n",
    "* n is the number of samples, \n",
    "* k is the number of independent variables (features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d539d3",
   "metadata": {},
   "source": [
    "### Implement random forest regression using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "065e900e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.08999999999999986\n",
      "Mean Squared Error: 0.008099999999999975\n",
      "Root Mean Squared Error: 0.08999999999999986\n",
      "R-squared: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bency\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:996: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f592bf2",
   "metadata": {},
   "source": [
    "## 10 Model Selection and Hyperparameter Tuning:\n",
    "\n",
    "Model selection and hyperparameter tuning are essential steps in building effective machine learning models. These processes help ensure that the chosen model generalizes well to new data points and performs optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef7e60",
   "metadata": {},
   "source": [
    "### Split datasets into training, validation, and testing sets:\n",
    "\n",
    "To evaluate a model's performance and tune hyperparameters, it's crucial to split the dataset into separate sets: training, validation, and testing. \n",
    "\n",
    "* The training set is used to fit the model,\n",
    "* the validation set is used to tune hyperparameters and select the best model, \n",
    "* testing set is used to evaluate the final model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c158f84",
   "metadata": {},
   "source": [
    "### Understand the concept of cross-validation:\n",
    "\n",
    "* Cross-validation is a technique used to assess the performance of a model by training and evaluating it on different subsets of the data. \n",
    "* The most common form of cross-validation is k-fold cross-validation, where the data is divided into k equal-sized folds. The model is trained on k-1 folds and evaluated on the remaining fold. This process is repeated k times, with each fold used as the validation set once. The average performance across all iterations is used as the model's performance estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5db0a",
   "metadata": {},
   "source": [
    "### Grid search and randomized search for hyperparameter tuning:\n",
    "\n",
    "Grid search and randomized search are two methods for hyperparameter tuning:\n",
    "\n",
    "* **Grid search**: An exhaustive search over a specified range of hyperparameter values. The model is trained and evaluated for each combination of hyperparameter values, and the combination that yields the best performance is chosen.\n",
    "\n",
    "* **Randomized search**: A more efficient alternative to grid search. Instead of trying every possible combination, a fixed number of random combinations of hyperparameter values are sampled. This approach can be faster and yield similar results compared to grid search, especially when dealing with a large number of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f4972",
   "metadata": {},
   "source": [
    "### Implement model selection and hyperparameter tuning using scikit-learn:\n",
    "    \n",
    "Scikit-learn provides various tools for model selection and hyperparameter tuning, such as train_test_split, cross_val_score, GridSearchCV, and RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63c2c2",
   "metadata": {},
   "source": [
    "### Example of using grid search for hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77fe97ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m load_boston()\n\u001b[0;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\__init__.py:157\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    109\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39m\n\u001b[0;32m    111\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124m        <https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\u001b[39m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72cf3e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best score: 14.858836289320982\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load data\n",
    "#data = load_boston()\n",
    "#X = data.data\n",
    "#y = data.target\n",
    "\n",
    "#having that boston datset was removed frm scikit-learn we need to load it manualy.\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "X = data\n",
    "y = target\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a random forest regressor\n",
    "rforest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Instantiate the grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rforest, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best combination of hyperparameters and their score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acff47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://lib.stat.cmu.edu/datasets/boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e9034",
   "metadata": {},
   "source": [
    "### Randomized search for hyperparameter tuning with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cde2d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 15, 'min_samples_split': 6, 'n_estimators': 33}\n",
      "Best score: 13.878459555873864\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, FKold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load data\n",
    "#having that boston datset was removed frm scikit-learn we need to load it manualy.\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "X = data\n",
    "y = target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 200),\n",
    "    'max_depth': randint(1, 30),\n",
    "    'min_samples_split': randint(2, 10)\n",
    "}\n",
    "\n",
    "# Create a random forest regressor\n",
    "rforest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Instantiate the randomized search with cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rforest, param_distributions=param_dist, n_iter=50, cv=kfold, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "# Fit the randomized search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best combination of hyperparameters and their score\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best score:\", -random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453fb4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6724a",
   "metadata": {},
   "source": [
    "## Assignment: Predicting House Prices using Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6dd21",
   "metadata": {},
   "source": [
    "### Objective: \n",
    "\n",
    "The goal of this assignment is to apply the concepts learned about regression models to predict house prices using a given dataset. You will use various regression models, evaluate their performance, and tune their hyperparameters.\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Use the Boston Housing dataset, which is available in scikit-learn's datasets module.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "* 1 Import necessary libraries and load the dataset.\n",
    "* 2 Perform exploratory data analysis and preprocessing (e.g., check for missing values, visualize the data, etc.).\n",
    "* 3 Split the dataset into training and testing sets.\n",
    "* 4 Implement the following regression models:\n",
    "\n",
    "    * Simple Linear Regression (choose an appropriate feature)\n",
    "    * Multiple Linear Regression\n",
    "    * Polynomial Regression\n",
    "    * Ridge Regression\n",
    "    * Lasso Regression\n",
    "    * Support Vector Regression\n",
    "    * Decision Tree Regression\n",
    "    * Random Forest Regression\n",
    "* 5 Evaluate the performance of each model using appropriate evaluation metrics, such as MAE, MSE, RMSE, R-squared, or Adjusted R-squared.\n",
    "* 6 Perform cross-validation and hyperparameter tuning for the models that require it, using grid search or randomized search.\n",
    "* 7 Compare the performance of the different models and discuss your findings.\n",
    "* 8 Choose the best model based on the evaluation metrics and provide insights into its performance and predictions.\n",
    "\n",
    "\n",
    "Submission: Prepare a report or Jupyter Notebook that includes your code, visualizations, results, and explanations for each step. The report should be well-organized, clear, and concise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4c7fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'hist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m\n\u001b[0;32m     27\u001b[0m df \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Check for missing values\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#df = pd.DataFrame(X, columns=boston.feature_names)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#df['target'] = y\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#print(df.isnull().sum())\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Visualize the data using histograms\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     35\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Visualize the data using scatter plots\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'hist'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load data\n",
    "#boston = load_boston()\n",
    "#X = boston.data\n",
    "#y = boston.target\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "X = data\n",
    "y = target\n",
    "df = data\n",
    "# Check for missing values\n",
    "#df = pd.DataFrame(X, columns=boston.feature_names)\n",
    "#df['target'] = y\n",
    "#print(df.isnull().sum())\n",
    "\n",
    "# Visualize the data using histograms\n",
    "df.hist(figsize=(12, 10))\n",
    "plt.show()\n",
    "\n",
    "# Visualize the data using scatter plots\n",
    "plt.scatter(df['RM'], df['target'])\n",
    "plt.xlabel('Average number of rooms per dwelling (RM)')\n",
    "plt.ylabel('Median value of owner-occupied homes in $1000s')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the data using a correlation matrix heatmap\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "# Implement Simple Linear Regression\n",
    "simple_lr = LinearRegression()\n",
    "simple_lr.fit(X_train[:, np.newaxis, 5], y_train) # Using the 'RM' feature (column index 5)\n",
    "simple_lr_metrics = evaluate_model(simple_lr, X_test[:, np.newaxis, 5], y_test)\n",
    "\n",
    "# Implement Multiple Linear Regression\n",
    "multiple_lr = LinearRegression()\n",
    "multiple_lr.fit(X_train, y_train)\n",
    "multiple_lr_metrics = evaluate_model(multiple_lr, X_test, y_test)\n",
    "\n",
    "# Implement Polynomial Regression (use the same Linear Regression model with transformed features)\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "poly_lr = LinearRegression()\n",
    "poly_lr.fit(X_train_poly, y_train)\n",
    "poly_lr_metrics = evaluate_model(poly_lr, X_test_poly, y_test)\n",
    "\n",
    "# Implement Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_metrics = evaluate_model(ridge, X_test, y_test)\n",
    "\n",
    "# Implement Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_metrics = evaluate_model(lasso, X_test, y_test)\n",
    "\n",
    "# Implement Support Vector Regression\n",
    "svr = SVR(kernel='linear', C=1.0)\n",
    "svr.fit(X_train, y_train)\n",
    "svr_metrics = evaluate_model(svr, X_test, y_test)\n",
    "\n",
    "# Implement Decision Tree Regression\n",
    "dtree = DecisionTreeRegressor()\n",
    "dtree.fit(X_train, y_train)\n",
    "dtree_metrics = evaluate_model(dtree,\n",
    "X_test, y_test)\n",
    "\n",
    "# Implement Random Forest Regression\n",
    "rforest = RandomForestRegressor(n_estimators=100)\n",
    "rforest.fit(X_train, y_train)\n",
    "rforest_metrics = evaluate_model(rforest, X_test, y_test)\n",
    "\n",
    "# Random Forest example\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 200),\n",
    "    'max_depth': randint(1, 30),\n",
    "    'min_samples_split': randint(2, 10)\n",
    "}\n",
    "\n",
    "rforest = RandomForestRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rforest, param_distributions=param_dist, n_iter=50, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters for Random Forest:\", random_search.best_params_)\n",
    "print(\"Best score for Random Forest:\", -random_search.best_score_)\n",
    "\n",
    "# Retrain the model with the best hyperparameters\n",
    "best_rforest = random_search.best_estimator_\n",
    "best_rforest.fit(X_train, y_train)\n",
    "best_rforest_metrics = evaluate_model(best_rforest, X_test, y_test)\n",
    "\n",
    "# Collect the evaluation metrics for each model in a DataFrame\n",
    "model_names = ['Simple LR', 'Multiple LR', 'Polynomial LR', 'Ridge', 'Lasso', 'SVR', 'Decision Tree', 'Random Forest', 'Tuned Random Forest']\n",
    "metrics_list = [simple_lr_metrics, multiple_lr_metrics, poly_lr_metrics, ridge_metrics, lasso_metrics, svr_metrics, dtree_metrics, rforest_metrics, best_rforest_metrics]\n",
    "\n",
    "results = pd.DataFrame(metrics_list, columns=['MAE', 'MSE', 'RMSE', 'R2'], index=model_names)\n",
    "print(results)\n",
    "\n",
    "# Visualize the performance of each model using a bar plot\n",
    "results.plot(kind='bar', figsize=(12, 8), ylabel='Metric Value', title='Model Performance Comparison')\n",
    "plt.show()\n",
    "\n",
    "# Select the best model (e.g., tuned random forest)\n",
    "best_model = best_rforest\n",
    "\n",
    "# Fit the best model on the entire dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Analyze feature importances for the Random Forest model\n",
    "importances = best_model.feature_importances_\n",
    "feature_importances = pd.Series(importances, index=boston.feature_names)\n",
    "feature_importances.sort_values(ascending=False).plot(kind='bar', figsize=(12, 8), ylabel='Importance', title='Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73590465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff5377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
